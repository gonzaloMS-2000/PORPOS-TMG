{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.models as models\n",
    "from biogeme.expressions import Beta, DefineVariable\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "full_df = pd.read_csv('../../../Data/SMTO_2019/SMTO_2019_Complete_Input.csv')\n",
    "school_codes = full_df['School'].unique().tolist()\n",
    "uni_codes = full_df[full_df['School_Type'] == 'University']['School'].unique().tolist()\n",
    "\n",
    "# Convert School column to numeric\n",
    "full_df['School'] = full_df['School'].apply(lambda x: school_codes.index(x))\n",
    "\n",
    "# Remove rows with missing information\n",
    "full_df = full_df.dropna(subset = ['Family'])\n",
    "full_df['Family'] = (full_df['Family'] * 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enrollment data\n",
    "enrol_df = pd.read_csv('../../../Data/School_Info_2019_Pred_Enrol.csv').set_index('Code')\n",
    "\n",
    "def code_to_log_enrol(code):\n",
    "    \"\"\"\n",
    "    Return natural logarithm of total enrollment of campus with given code\n",
    "    If code is invalid, raise KeyError\n",
    "    If no enrollment information available for that code, return np.nan\n",
    "    \"\"\"\n",
    "    return math.log(enrol_df.loc[code]['Total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(cm):\n",
    "    \"\"\"\n",
    "    Given confusion matrix as 2D array, return accuracy\n",
    "    \"\"\"\n",
    "    correct = sum([cm[i][i] for i in range(len(cm))])\n",
    "    return correct/sum(sum(cm,[])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(probs, hardmax):\n",
    "    cm = []\n",
    "    if hardmax:\n",
    "        for school in range(len(school_codes)):\n",
    "            cm.append([(probs[full_df['School'] == school][['Prob.' + i for i in school_codes]].idxmax(axis = 1) == 'Prob.' + j).sum() for j in school_codes])\n",
    "    else:\n",
    "        for school in range(len(school_codes)):\n",
    "            cm.append((probs[full_df['School'] == school][['Prob.' + i for i in school_codes]].sum().values.tolist()))    \n",
    "    return cm\n",
    "\n",
    "def get_accuracy(cm):\n",
    "    correct = sum([cm[i][i] for i in range(len(school_codes))])\n",
    "    return correct/sum(sum(cm,[])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['School', 'Family'] + ['Dist.' + code for code in school_codes]\n",
    "database = db.Database(\"SMTO_2019\", full_df[cols_to_keep])\n",
    "unis, cols = [], []\n",
    "V, av = {}, {}\n",
    "B_DIST = Beta('B_DIST', 0, None, None, 0)\n",
    "B_FAM_DIST = Beta('B_FAM_DIST', 0, None, None, 0)\n",
    "UNI = Beta('UNI', 3, 0.0000001, 10, 0)\n",
    "COL = Beta('COL', 3, 0.0000001, 10, 0)\n",
    "\n",
    "for i in range(len(school_codes)):\n",
    "    code = school_codes[i]\n",
    "    if code in uni_codes:\n",
    "        unis.append(i)\n",
    "    else:\n",
    "        cols.append(i)\n",
    "    V[i] = code_to_log_enrol(code) + database.variables['Dist.' + code] * (B_DIST + B_FAM_DIST * database.variables['Family'])\n",
    "    av[i] = 1\n",
    "\n",
    "UNIS = (UNI, unis)\n",
    "COLS = (COL, cols)\n",
    "nests = UNIS, COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:16:52] < General >   Remove 0 unused variables from the database as only 29 are used.\n",
      "[17:16:53] < General >   Log likelihood (N=11005):  -27927.74\n",
      "[17:16:53] < General >   Minimize with tol 1e-07\n",
      "[17:16:54] < General >   Log likelihood (N=11005):  -27927.74 Gradient norm:      2e+05  \n",
      "[17:17:01] < General >   Log likelihood (N=11005):    -319285 Gradient norm:      4e+05  \n",
      "[17:17:07] < General >   Log likelihood (N=11005):  -48152.19 Gradient norm:      3e+05  \n",
      "[17:17:13] < General >   Log likelihood (N=11005):  -27188.85 Gradient norm:      8e+04  \n",
      "[17:17:18] < General >   Log likelihood (N=11005):  -26754.21 Gradient norm:      5e+04  \n",
      "[17:17:23] < General >   Log likelihood (N=11005):  -26603.74 Gradient norm:      3e+04  \n",
      "[17:17:29] < General >   Log likelihood (N=11005):  -26561.99 Gradient norm:      9e+03  \n",
      "[17:17:35] < General >   Log likelihood (N=11005):  -26554.14 Gradient norm:      7e+03  \n",
      "[17:17:41] < General >   Log likelihood (N=11005):   -26537.8 Gradient norm:      6e+03  \n",
      "[17:17:48] < General >   Log likelihood (N=11005):  -26532.02 Gradient norm:      4e+03  \n",
      "[17:17:55] < General >   Log likelihood (N=11005):  -26522.22 Gradient norm:      5e+03  \n",
      "[17:18:03] < General >   Log likelihood (N=11005):  -26509.88 Gradient norm:      9e+03  \n",
      "[17:18:10] < General >   Log likelihood (N=11005):  -26479.73 Gradient norm:      1e+04  \n",
      "[17:18:17] < General >   Log likelihood (N=11005):  -26423.42 Gradient norm:      2e+04  \n",
      "[17:18:23] < General >   Log likelihood (N=11005):  -26350.71 Gradient norm:      2e+04  \n",
      "[17:18:29] < General >   Log likelihood (N=11005):  -26296.74 Gradient norm:      7e+03  \n",
      "[17:18:34] < General >   Log likelihood (N=11005):  -26278.81 Gradient norm:      1e+03  \n",
      "[17:18:40] < General >   Log likelihood (N=11005):     -26278 Gradient norm:      3e+02  \n",
      "[17:18:45] < General >   Log likelihood (N=11005):  -26277.94 Gradient norm:      2e+02  \n",
      "[17:18:50] < General >   Log likelihood (N=11005):  -26277.72 Gradient norm:      1e+03  \n",
      "[17:18:55] < General >   Log likelihood (N=11005):  -26277.38 Gradient norm:      1e+03  \n",
      "[17:19:00] < General >   Log likelihood (N=11005):  -26275.14 Gradient norm:      4e+03  \n",
      "[17:19:04] < General >   Log likelihood (N=11005):   -26272.6 Gradient norm:      3e+03  \n",
      "[17:19:08] < General >   Log likelihood (N=11005):  -26269.49 Gradient norm:      5e+02  \n",
      "[17:19:13] < General >   Log likelihood (N=11005):  -26269.24 Gradient norm:      2e+02  \n",
      "[17:19:17] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:          2  \n",
      "[17:19:21] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:        0.2  \n",
      "[17:19:26] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:        0.2  \n",
      "[17:19:30] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:        0.2  \n",
      "[17:19:35] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:       0.04  \n",
      "[17:19:39] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:19:43] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      9e-05  \n",
      "[17:19:47] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:19:51] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:19:55] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:19:59] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:03] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:07] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:11] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:15] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:18] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:22] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:26] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:30] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:35] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:40] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      3e-06  \n",
      "[17:20:44] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:49] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:53] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:20:57] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:04] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:08] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:12] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:17] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:22] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:27] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:32] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:37] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:42] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      1e-06  \n",
      "[17:21:47] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:52] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:21:56] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:01] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:06] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:12] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:16] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:21] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:26] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:31] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:36] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:41] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:46] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:52] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:22:56] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:02] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:08] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:15] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:22] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:28] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:        0.9  \n",
      "[17:23:32] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:     0.0005  \n",
      "[17:23:36] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:40] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:44] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:48] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:52] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:23:56] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:01] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:05] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:09] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:24:13] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:18] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:22] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:26] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:31] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:36] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:42] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:48] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:53] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002  \n",
      "[17:24:59] < General >   Log likelihood (N=11005):  -26269.23 Gradient norm:      0.002 Hessian norm:       7e+06 BHHH norm:       2e+07\n",
      "[17:25:00] < General >   Results saved in file biogemeModelDefaultName.html\n",
      "[17:25:00] < General >   Results saved in file biogemeModelDefaultName.pickle\n"
     ]
    }
   ],
   "source": [
    "import biogeme.messaging as msg\n",
    "logprob = models.lognested(V, av, nests, database.variables[\"School\"])\n",
    "logger = msg.bioMessage()\n",
    "logger.setGeneral()\n",
    "\n",
    "biogeme = bio.BIOGEME(database, logprob)\n",
    "results = biogeme.estimate()\n",
    "pandasResults = results.getEstimatedParameters()\n",
    "betas = results.getBetaValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Std err</th>\n",
       "      <th>t-test</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Rob. Std err</th>\n",
       "      <th>Rob. t-test</th>\n",
       "      <th>Rob. p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_DIST</th>\n",
       "      <td>-0.020911</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-29.413694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>-14.893980</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_FAM_DIST</th>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>2.589811</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>1.351673</td>\n",
       "      <td>0.17648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COL</th>\n",
       "      <td>1.001433</td>\n",
       "      <td>0.018886</td>\n",
       "      <td>53.025576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>44.788973</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNI</th>\n",
       "      <td>0.813110</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>74.724932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>72.368137</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Value   Std err     t-test   p-value  Rob. Std err  \\\n",
       "B_DIST     -0.020911  0.000711 -29.413694  0.000000      0.001404   \n",
       "B_FAM_DIST  0.002281  0.000881   2.589811  0.009603      0.001688   \n",
       "COL         1.001433  0.018886  53.025576  0.000000      0.022359   \n",
       "UNI         0.813110  0.010881  74.724932  0.000000      0.011236   \n",
       "\n",
       "            Rob. t-test  Rob. p-value  \n",
       "B_DIST       -14.893980       0.00000  \n",
       "B_FAM_DIST     1.351673       0.17648  \n",
       "COL           44.788973       0.00000  \n",
       "UNI           72.368137       0.00000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:25:00] < General >   Remove 1 unused variables from the database as only 28 are used.\n",
      "Hardmax Accuracy: 25.50 %\n",
      "Softmax Accuracy: 15.21 %\n"
     ]
    }
   ],
   "source": [
    "simulate = {'Prob.' + school_codes[i]: models.logit(V, av, i) for i in range(len(school_codes))}\n",
    "sim_biogeme = bio.BIOGEME(database, simulate)\n",
    "probs = sim_biogeme.simulate(betas).set_index(full_df.index)\n",
    "hard_cm = get_cm(probs, True)\n",
    "soft_cm = get_cm(probs, False)\n",
    "print(\"Hardmax Accuracy: {:2.2f} %\".format(get_accuracy(hard_cm)))\n",
    "print(\"Softmax Accuracy: {:2.2f} %\".format(get_accuracy(soft_cm)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
